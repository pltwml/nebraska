{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Lecture 5\n",
    "=======\n",
    "\n",
    "Outline\n",
    "---------\n",
    "\n",
    "### I Introduction to Keras\n",
    "### II Prominent Machine Learning problem classes revisited in Keras\n",
    "> A. MNIST in DenseNet\n",
    "> \n",
    "> B. Binary Classsification\n",
    ">\n",
    "> C. Multiclass Single Label Classification\n",
    ">\n",
    "> D. Scalar Regression\n",
    ">\n",
    "> E. Regularization in Keras\n",
    "\n",
    "### III Introduction to ConvNets\n",
    "> A. Structures and motivations behind the ConvNet architecture\n",
    ">\n",
    "> B. The use case: MNIST\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sidekick: Pójdźże, kiń tę chmurność w głąb flaszy!\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. The first four lectures covered fundamental and universal concepts of Deep Neural Networks\n",
    "### 2. The second part of the course will take mostly pragmatic approach\n",
    "### 3. We would like to move towards high level applications and cover major advanced Deep Learning architectures and domains:\n",
    "> A. Computer Vision: ConvNets\n",
    "> \n",
    "> B. Texts and sequences: RNN\n",
    ">\n",
    "> C. Generative Deep Learning: LSTM, VAE, GAN, DeepDream, Neural Style Transfer\n",
    ">\n",
    "> D. Contemporary topics, like Reinforcement Learning, Attention, ...\n",
    "### 4. We are changing PyTorch for higher level framework: Keras\n",
    "### 5. We shall think of using Keras as a tool and cover relevant mathematics whenever possible\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "I Introduction to Keras\n",
    "-----------------------\n",
    "\n",
    "### A. Installation: \n",
    "\n",
    "> 1. conda install -c anaconda keras-gpu \n",
    ">\n",
    ">or\n",
    ">\n",
    "> 2. pip install keras\n",
    "\n",
    "### B. Importing submodules to notebook:\n",
    "> from keras import ...\n",
    "\n",
    "### C. What is it?\n",
    "> Keras is a high level Deep Learning framework based on TensorFlow and supporting GPU\n",
    ">\n",
    "> Designed for rapid prototyping in AI research, took leading role on e.g. Kaggle\n",
    ">\n",
    "> It offers two development modes: \n",
    ">> Out of the box models and layers\n",
    ">>\n",
    ">> Low level functional API for advanced use \n",
    "> \n",
    "### D. Let us reconsider a few of the major Machine Learning problem classes using Keras\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "II A: MNIST digits classification using fully connected layers\n",
    "---------------------\n",
    "\n",
    "### The problem: classify handwritten digits\n",
    "> ### What kind of a problem is it? \n",
    ">\n",
    ">>10-class single label image classification\n",
    "> ### What is the dataset? \n",
    ">\n",
    ">> 60000 (training+validation) + 10000 (testing) grayscale pictures stored in 28 x 28 x 1 3D tensors\n",
    ">\n",
    "> ### What is the hypothesis?\n",
    ">> Fully connected layers, vectorized input data and 10 softmax outputs\n",
    ">\n",
    "> ### Why do we consider this case?\n",
    ">> To show off Keras simplicity and superiority of Convolutional Networks behave in later comparison\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "II B: IMDB dataset\n",
    "---------------------\n",
    "\n",
    "### The problem: determine if movies reviews are positive or negative\n",
    "> ### What kind of a problem is it? \n",
    ">\n",
    ">> Binary variable length text sentiment classification\n",
    "> ### What is the dataset? \n",
    ">\n",
    ">> 25000 (training+validation) + 25000 (testing) multihot, vectorized, english reviews\n",
    ">\n",
    "> ### What is the hypothesis?\n",
    ">> Fully connected layers with sigmoid output and binary_crossentropy\n",
    ">\n",
    "> ### Why do we consider this case?\n",
    ">> To show text data vectorization and dense layers binary classification\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "II C: The Reuters dataset\n",
    "---------------------\n",
    "\n",
    "### The problem: classify short newswires on various topics\n",
    "> ### What kind of a problem is it? \n",
    ">\n",
    ">> Variable length text topic classification into 46 excluding categories\n",
    "> ### What is the dataset? \n",
    ">\n",
    ">> 8982 (training+validation) + 2246 (testing) multihot, vectorized, english reviews\n",
    ">\n",
    "> ### What is the hypothesis?\n",
    ">> Fully connected layers with 46 softmax outputs and categorical_crossentropy\n",
    ">\n",
    "> ### Why do we consider this case?\n",
    ">> To show text data vectorization and dense layers multilabel classification\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "II D: Boston Housing Price Dataset\n",
    "---------------------\n",
    "\n",
    "### The problem: predict real estate value\n",
    "> ### What kind of a problem is it? \n",
    ">\n",
    ">> Scalar regression\n",
    "> ### What is the dataset? \n",
    ">\n",
    ">> Small set of 404 (training+validation) and 102 (testing) 13-feature numerical vectors\n",
    ">\n",
    "> ### What is the hypothesis?\n",
    ">> Fully connected layers with linear output and mse loss\n",
    ">\n",
    "> ### Why do we consider this case?\n",
    ">> To show small dataset regression with k-fold validation and dataset normalization\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "II E: IMDB dataset and Regularization\n",
    "---------------------\n",
    "\n",
    "### The problem: DenseNet Overfitting!\n",
    ">\n",
    "> ### Why are we reconsidering IMDB case?\n",
    ">> To show regularization and dropout in Keras\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "III Introducing Convolutional Networks\n",
    "----------------\n",
    "\n",
    "### This Lecture: an intuitive and practical introduction, formal description left for the next week\n",
    "\n",
    "### A: New (old) Convolutional Networks: structures and motivations behind the ConvNet architecture:\n",
    "> Inspired by visual signals processing by the brain\n",
    ">\n",
    "> Sparse architecture: radically reduced spato-temporal complexity\n",
    ">\n",
    "> Locality: focus on local, granular structures\n",
    ">\n",
    "> Capable of developing invariances (w/r to translations, rotations, etc.)\n",
    ">\n",
    "> Capable of capturing hierarchies of features and complex strucutres\n",
    ">\n",
    "> Less prone to overfitting (yet still!)\n",
    ">\n",
    "> Not only for visual data! E.g. text, sound.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### A. The idea: hierarchical image decomposition preserving local spatial correlations:\n",
    "\n",
    "![Image](gfx/cnn-cat-assembly.jpg \"CNN Cat Assembly\")\n",
    "\n",
    "### B. In practise, introduce local receptive fields: patches observing small portions of the feature map\n",
    "\n",
    "![Image](gfx/cnn-action.jpg \"CNN Action\")\n",
    "\n",
    "### C. Basic ConvNet stack for images\n",
    "\n",
    "> ### 1. Start from 3D (not 1D) input image tensor (Height, Width, Channels): (28, 28, 1) or (H, W, 3) for RGB\n",
    ">\n",
    "> ### 2. Introduce new elements: many small \"convolutional kernels\" defining local trainable feature filters, typically shaped (3, 3) or (5, 5). There can be great many of them in the given layer.\n",
    ">\n",
    "> ### 3. Scan each pixel of the image with kernels and produce corresponding new output map pixel channels\n",
    ">\n",
    "> ### 4. Downsample image by MaxPooling: select the most prominent local features\n",
    "> \n",
    "> ### 5. Repeat layer stack as many times as needed\n",
    ">\n",
    "> ### 6. Introduce Dense layers and output to construct classifier mapping to cathegories\n",
    "\n",
    "### D. Convolution action in detail (no pooling: HxW size from valid pixels)\n",
    "\n",
    "![Image](gfx/cnn-conv-layer-decomposition.png \"CNN Conv Layer Decomposition\")\n",
    "\n",
    "### E. Local patches: valid convolution points\n",
    "\n",
    "![Image](gfx/cnn-conv-padding.png \"CNN Valid Convolution\")\n",
    "\n",
    "### F. Padding for boundary effects mitigation with (5, 5) kernel\n",
    "\n",
    "![Image](gfx/cnn-conv-padding2.png \"CNN Padding\")\n",
    "\n",
    "### G. Receptive field action: same size, filtered output\n",
    "\n",
    "![Image](gfx/cnn-receptive-field.jpg \"CNN receptive field\")\n",
    "\n",
    "### H. Strides: \n",
    "> ### Downsampling, used with pooling and sometimes with kernels\n",
    ">\n",
    "> ### This step is crucial for reducing network parameters count and allowing for wide scope of the deep receptive fields: 'hyperbolic' vs linear structure of the network (like AdS space!)\n",
    ">\n",
    "> ### Translation invariance and global correlations appear due to the locally maximal feature selection\n",
    "\n",
    "![Image](gfx/cnn-conv-stride.png \"CNN Stride - Downsampling\")\n",
    "\n",
    "### I. Cat image activations in a deeper (4th) layer\n",
    "![Image](gfx/cnn-4th-layer-activations.png \"4th layer activations\")\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### B: MNIST use case\n",
    "> ### This network reduces error by over 50% relative to the DenseNet considered earler\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 TF 1.2 DEV",
   "language": "python",
   "name": "devtf36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
